ì „í˜•ì ì¸ RAG (ë­ì²´ì¸ ë²„ì „)
==================================

.. admonition:: ë­ì²´ì¸ ë²„ì „ì€ ë‹¤ìŒ íŠœí† ë¦¬ì–¼ì—ì„œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
   :class: attention

   ë§ì€ ì‘ì› ë¶€íƒë“œë¦½ë‹ˆë‹¤. ğŸ˜‰

ì§€ì‹ Load ë° Split
-----------------------------

.. code-block:: bash
   :caption: ì˜ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

   pip install -U langchain langchain-community langchain-text-splitters

.. code-block:: python
   :linenos:

   from pprint import pprint

   from langchain_community.document_loaders import TextLoader
   from langchain_text_splitters import RecursiveCharacterTextSplitter

   # Load ë‹¨ê³„
   doc_list = TextLoader(file_path="./ë¹½ë‹¤ë°©.txt").load()
   print(f"loaded {len(doc_list)} documents")  # 1

   # Split ë‹¨ê³„
   text_splitter = RecursiveCharacterTextSplitter(
       chunk_size=140,  # ë¬¸ì„œë¥¼ ë‚˜ëˆŒ ìµœì†Œ ê¸€ì ìˆ˜ (ë””í´íŠ¸: 4000)
       chunk_overlap=0,  # ë¬¸ì„œë¥¼ ë‚˜ëˆŒ ë•Œ ê²¹ì¹˜ëŠ” ê¸€ì ìˆ˜ (ë””í´íŠ¸: 200)
   )
   doc_list = text_splitter.split_documents(doc_list)
   print(f"split into {len(doc_list)} documents")  # 9

   pprint(doc_list)

``RecursiveCharacterTextSplitter`` ì—ì„œëŠ” êµ¬ë¶„ì (ë””í´íŠ¸ ``["\n\n", "\n", " ", ""]``)ë¡œ ë‚˜ëˆ„ê³ 
``chunk_size`` í¬ê¸° ë§Œí¼ ë¬¸ì„œë¥¼ ëª¨ìœ¼ê³ , ì´ì–´ì§€ëŠ” ë¬¸ì„œëŠ” ``chunk_overlap`` ë§Œí¼ ê²¹ì³ì„œ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

"ë¹½ë‹¤ë°©.txt" íŒŒì¼ì—ì„œ ê° ë©”ë‰´ë“¤ì´ êµ¬ë¶„ì ``\n\n``\ë¡œ ë‚˜ëˆ ì§€ë‹ˆê¹Œ 10ê°œ ë¬¸ì„œë¡œ ë‚˜ëˆ ì§€ì§€ë§Œ,
ê° ë¬¸ì„œì˜ ê¸¸ì´ê°€ ``[98, 68, 52, 68, 114, 126, 81, 105, 65, 83]`` ì´ê³ ,
ë‘ë²ˆì§¸ ë¬¸ì„œëŠ” 68ì, ì„¸ë²ˆì§¸ ë¬¸ì„œëŠ” 52ìë¡œì„œ ``chunk_size=140`` ë³´ë‹¤ ì‘ì€ ê°’ì´ë¼ í•œ ë¬¸ì„œë¡œ ë¬¶ì—¬ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.

ê·¸ë˜ì„œ 10ê°œì˜ ë¬¸ì„œê°€ ì•„ë‹ˆë¼, 9ê°œì˜ ë¬¸ì„œë¡œë§Œ ë‚˜ëˆ ì§„ ìƒí™©ì…ë‹ˆë‹¤. 2ë²ˆ ë©”ë‰´ì™€ 3ë²ˆ ë©”ë‰´ê°€ ë¬¶ì—¬ìˆë„¤ìš”. ğŸ¤”

.. code-block:: text
   :emphasize-lines: 4

   loaded 1 documents
   split into 9 documents
   [Document(metadata={'source': './ë¹½ë‹¤ë°©.txt'}, page_content='1. ì•„ì´ìŠ¤í‹°ìƒ·ì¶”ê°€(ì•„.ìƒ·.ì¶”)\n  - SNSì—ì„œ ë” ìœ ëª…í•œ ê¿€íŒ ì¡°í•© ìŒë£Œ :) ìƒì½¤ë‹¬ì½¤í•œ ë³µìˆ­ì•„ë§› ì•„ì´ìŠ¤í‹°ì— ì§„í•œ ì—ìŠ¤í”„ë ˆì†Œ ìƒ·ì´ ì–´ìš°ëŸ¬ì ¸ í™˜ìƒì¡°í•©\n  - ê°€ê²©: 3800ì›'),
    Document(metadata={'source': './ë¹½ë‹¤ë°©.txt'}, page_content='2. ë°”ë‹ë¼ë¼ë–¼(ICED)\n  - ë¶€ë“œëŸ¬ìš´ ìš°ìœ ì™€ ë‹¬ì½¤í•˜ê³  ì€ì€í•œ ë°”ë‹ë¼ê°€ ì¡°í™”ë¥¼ ì´ë£¨ëŠ” ìŒë£Œ\n  - ê°€ê²©: 4200ì›\n\n3. ì‚¬ë¼ë‹¤ë¹µ\n  - ë¹½ë‹¤ë°©ì˜ ëŒ€í‘œë©”ë‰´ :) ì¶”ì–µì˜ ê°ì ì‚¬ë¼ë‹¤ë¹µ\n  - ê°€ê²©: 3900ì›'),
    Document(metadata={'source': './ë¹½ë‹¤ë°©.txt'}, page_content='4. ë¹½ì‚¬ì´ì¦ˆ ì•„ë©”ë¦¬ì¹´ë…¸(ICED)\n  - ì—ìŠ¤í”„ë ˆì†Œ 4ìƒ·ì´ ë“¤ì–´ê°€ ê¹Šê³  ì§„í•œ ë§›ì˜ ì•„ë©”ë¦¬ì¹´ë…¸\n  - ê°€ê²©: 3500ì›'),
    Document(metadata={'source': './ë¹½ë‹¤ë°©.txt'}, page_content='5. ë¹½ì‚¬ì´ì¦ˆ ì›ì¡°ì»¤í”¼(ICED)\n  - ë¹½ë‹¤ë°©ì˜ BESTë©”ë‰´ë¥¼ ë” í¬ê²Œ ì¦ê²¨ë³´ì„¸ìš” :) [ì£¼ì˜. 564mg ê³ ì¹´í˜ì¸ìœ¼ë¡œ ì¹´í˜ì¸ì— ë¯¼ê°í•œ ì–´ë¦°ì´, ì„ì‚°ë¶€ëŠ” ì„­ì·¨ì— ì£¼ì˜ë°”ëë‹ˆë‹¤]\n  - ê°€ê²©: 4000ì›'),
    Document(metadata={'source': './ë¹½ë‹¤ë°©.txt'}, page_content='6. ë¹½ì‚¬ì´ì¦ˆ ì›ì¡°ì»¤í”¼ ì œë¡œìŠˆê±°(ICED)\n  - ë¹½ë‹¤ë°©ì˜ BESTë©”ë‰´ë¥¼ ë” í¬ê²Œ, ì œë¡œìŠˆê±°ë¡œ ì¦ê²¨ë³´ì„¸ìš” :) [ì£¼ì˜. 686mg ê³ ì¹´í˜ì¸ìœ¼ë¡œ ì¹´í˜ì¸ì— ë¯¼ê°í•œ ì–´ë¦°ì´, ì„ì‚°ë¶€ëŠ” ì„­ì·¨ì— ì£¼ì˜ë°”ëë‹ˆë‹¤]\n  - ê°€ê²©: 4000ì›'),
    Document(metadata={'source': './ë¹½ë‹¤ë°©.txt'}, page_content='7. ë¹½ì‚¬ì´ì¦ˆ ë‹¬ì½¤ì•„ì´ìŠ¤í‹°(ICED)\n  - ë¹½ë‹¤ë°©ì˜ BESTë©”ë‰´ë¥¼ ë” í¬ê²Œ ì¦ê²¨ë³´ì„¸ìš” :) ì‹œì›í•œ ë³µìˆ­ì•„ë§› ì•„ì´ìŠ¤í‹°\n  - ê°€ê²©: 4300ì›'),
    Document(metadata={'source': './ë¹½ë‹¤ë°©.txt'}, page_content='8. ë¹½ì‚¬ì´ì¦ˆ ì•„ì´ìŠ¤í‹°ìƒ·ì¶”ê°€(ICED)\n  - SNSì—ì„œ ë” ìœ ëª…í•œ ê¿€íŒ ì¡°í•© ìŒë£Œ :) ìƒì½¤ë‹¬ì½¤í•œ ë³µìˆ­ì•„ë§› ì•„ì´ìŠ¤í‹°ì— ì§„í•œ ì—ìŠ¤í”„ë ˆì†Œ 2ìƒ·ì´ ì–´ìš°ëŸ¬ì ¸ í™˜ìƒì¡°í•©\n  - ê°€ê²©: 4800ì›'),
    Document(metadata={'source': './ë¹½ë‹¤ë°©.txt'}, page_content='9. ë¹½ì‚¬ì´ì¦ˆ ì•„ì´ìŠ¤í‹° ë§ê³ ì¶”ê°€+ë…¸ë€ë¹¨ëŒ€\n  - SNSí•«ë©”ë‰´ ì•„ì´ìŠ¤í‹°ì— ë§ê³ ë¥¼ í•œê°€ë“:)\n  - ê°€ê²©: 6300ì›'),
    Document(metadata={'source': './ë¹½ë‹¤ë°©.txt'}, page_content='10. ë¹½ì‚¬ì´ì¦ˆ ì´ˆì½”ë¼ë–¼(ICED)\n  - ë¹½ë‹¤ë°©ì˜ BESTë©”ë‰´ë¥¼ ë” í¬ê²Œ ì¦ê²¨ë³´ì„¸ìš” :) ì§„ì§œ~ì™„~ì „ ì§„í•œ ì´ˆì½”ë¼ë–¼\n  - ê°€ê²© : 5500ì›')]

ì „ì²´ ì½”ë“œ
-------------------

VectorStoreëŠ” `FAISS <https://python.langchain.com/docs/integrations/vectorstores/faiss/>`_\ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.
Facebook AI Researchì—ì„œ ê°œë°œí•œ íš¨ìœ¨ì ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œë„ ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ë©°, ë©”ëª¨ë¦¬ì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ë””ìŠ¤í¬ì— ì €ì¥/ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

.. code-block:: bash
   :caption: ì˜ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

   pip install -U langchain langchain-community langchain-openai langchain-text-splitters faiss-cpu tiktoken

.. code-block:: python
   :linenos:

   import os.path
   from pprint import pprint
   from typing import List
   from uuid import uuid4

   import faiss
   from langchain.chains.llm import LLMChain
   from langchain.chains.retrieval_qa.base import RetrievalQA
   from langchain_community.docstore import InMemoryDocstore
   from langchain_community.document_loaders import TextLoader
   from langchain_core.messages import AIMessage
   from langchain_core.prompts import PromptTemplate
   from langchain_core.runnables import RunnableLambda
   from langchain_core.vectorstores import VectorStore
   from langchain_openai import ChatOpenAI
   from langchain_openai.embeddings import OpenAIEmbeddings
   from langchain_community.vectorstores import FAISS
   from langchain_text_splitters import RecursiveCharacterTextSplitter

   faiss_folder_path = "faiss_index"

   embedding = OpenAIEmbeddings(model="text-embedding-3-small")


   def get_vector_store() -> VectorStore:
       if not os.path.exists(faiss_folder_path):
           doc_list = TextLoader(file_path="./ë¹½ë‹¤ë°©.txt").load()
           print(f"loaded {len(doc_list)} documents")  # 1

           text_splitter = RecursiveCharacterTextSplitter(
               chunk_size=140,
               chunk_overlap=0,
               length_function=len,
               is_separator_regex=True,
           )
           doc_list = text_splitter.split_documents(doc_list)
           print(f"split into {len(doc_list)} documents")  # 9

           ì°¨ì›ìˆ˜ = len(embedding.embed_query("hello"))  # 1536
           # ì°¨ì›ìˆ˜ = 1536

           index = faiss.IndexFlatL2(ì°¨ì›ìˆ˜)

           vector_store = FAISS(
               embedding_function=embedding,
               index=index,
               docstore=InMemoryDocstore(),
               index_to_docstore_id={},
           )

           uuids = [str(uuid4()) for _ in range(len(doc_list))]
           vector_store.add_documents(documents=doc_list, ids=uuids)

           vector_store.save_local("faiss_index")
       else:
           vector_store = FAISS.load_local(
               faiss_folder_path,
               embedding,
               allow_dangerous_deserialization=True,
           )

       return vector_store


   def main():
       vector_store = get_vector_store()

       question = "ë¹½ë‹¤ë°© ì¹´í˜ì¸ì´ ë†’ì€ ìŒë£Œì™€ ê°€ê²©ì€?"

       # ì§ì ‘ similarity_search ë©”ì„œë“œ í˜¸ì¶œì„ í†µí•œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
       # search_doc_list = vector_store.similarity_search(question)
       # pprint(search_doc_list)

       # retriever ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
       # retriever = vector_store.as_retriever()
       # search_doc_list = retriever.invoke(question)
       # pprint(search_doc_list)

       # Chainì„ í†µí•œ retriever ìë™ í˜¸ì¶œ
       # llm = ChatOpenAI(model_name="gpt-4o-mini")
       # retriever = vector_store.as_retriever()
       # qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
       # ai_message = qa_chain.invoke(question)
       # print("[AI]", ai_message["result"])  # keys: "query", "result"

       llm = ChatOpenAI(model_name="gpt-4o-mini")
       retriever = vector_store.as_retriever()
       prompt_template = PromptTemplate(
           template="Context: {context}\n\nQuestion: {question}\n\nAnswer:",
           input_variables=["context", "question"],
       )

       rag_pipeline = (
           RunnableLambda(
               # ì•„ë˜ invokeë¥¼ í†µí•´ ì „ë‹¬ë˜ëŠ” ê°’ì´ ì¸ìë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
               lambda x: {
                   "context": retriever.invoke(x),
                   "question": x,
               }
           )
           | prompt_template
           | llm
       )
       ai_message: AIMessage = rag_pipeline.invoke(question)
       print("[AI]", ai_message.content)  # AIMessage íƒ€ì…
       print(ai_message.usage_metadata)


   if __name__ == "__main__":
       main()

ì‹¤í–‰ ê²°ê³¼
-----------------

.. code-block:: text

   [AI] ë¹½ë‹¤ë°©ì—ì„œ ì¹´í˜ì¸ì´ ë†’ì€ ìŒë£Œì™€ ê·¸ ê°€ê²©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

   1. **ë¹½ì‚¬ì´ì¦ˆ ì›ì¡°ì»¤í”¼(ICED)**  
      - ì¹´í˜ì¸: 564mg  
      - ê°€ê²©: 4000ì›  

   2. **ë¹½ì‚¬ì´ì¦ˆ ì›ì¡°ì»¤í”¼ ì œë¡œìŠˆê±°(ICED)**  
      - ì¹´í˜ì¸: 686mg  
      - ê°€ê²©: 4000ì›  

   ì´ ë‘ ìŒë£ŒëŠ” ì¹´í˜ì¸ í•¨ëŸ‰ì´ ë†’ìœ¼ë¯€ë¡œ, ì¹´í˜ì¸ì— ë¯¼ê°í•œ ì–´ë¦°ì´ì™€ ì„ì‚°ë¶€ëŠ” ì„­ì·¨ì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
   {'input_tokens': 499, 'output_tokens': 132, 'total_tokens': 631, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}

ë­ì²´ì¸/ë­ê·¸ë˜í”„ ë²„ì „ë„ ê¸°ëŒ€í•´ì£¼ì„¸ìš”. ğŸ¥³
