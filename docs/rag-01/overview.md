# LLM은 어떤 질문이든 답변을 할려고 노력합니다.

LLM은 가지고 있는 지식에 기반해서 답변을 합니다.

## LLM은 몰라도 어떻게든 짜내어 답변

LLM은 검색엔진이 아닙니다. 전 세계 모든 지식을 알고 있지 못합니다. 알고 있는 지식 범위내에서 어떻게든 짜내어 답변을 합니다.
사실과 의견을 구분해달라고 요청하거나 모르는 건 모른다고 대답하라고 요청하지 않는다면,
지식이 부족한 질문에는 환각 (Hallucination)이나 틀린 정보 혹은 모호한 답변을 할 수 밖에 없습니다. 사람도 마찬가지죠.

```{admonition} 질문 : 빽다방 카페인이 높은 음료와 가격은?
:class: warning

빽다방의 카페인 함량이 높은 음료는 일반적으로 아메리카노, 카페라떼, 그리고 에스프레소와 같은 커피 기반 음료입니다. 가격은 지역이나 매장에 따라 다를 수 있지만, 대체로 아메리카노는 3,000원에서 4,000원 사이, 카페라떼는 4,000원에서 5,000원 사이, 에스프레소는 2,500원에서 3,500원 사이로 판매됩니다. 정확한 가격과 카페인 함량은 빽다방 공식 웹사이트나 매장에서 확인하는 것이 가장 좋습니다.
```

## 파인 튜닝

[Fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)은 기존 LLM 모델에 새로운 지식을 학습시켜 새로운 모델을 만드는 방법입니다. 기존 모델에 추가 데이터셋으로 재학습을 수행하여 특정 도메인이나 작업에 특화된 모델을 만들 수 있습니다. 본 튜토리얼에서는 다루지 않습니다.

+ 장점
    - 특정 도메인에 대한 전문성 향상
    - 일관된 답변 제공 가능
    - 모델 크기 증가 없이 새로운 지식 추가

+ 단점
    - 많은 양의 고품질 학습 데이터 필요
    - 학습에 상당한 시간과 비용 소요
    - 새로운 지식 추가 시 재학습 필요

```{admonition} 파인튜닝 모델 API 가격

OpenAI 에서는 파인튜닝된 모델은 1.5배 비싼 API 호출 가격이 적용됩니다.
```

## 프롬프트에 지식을 보강

Perplexity에서는 검색을 통해 충분한 지식을 확보한 후에 답변을 생성합니다.
최근 ChatGPT 서비스에서도 최근 검색 기능이 추가되어, 답변 전에 검색을 통해 지식을 보강할 수 있습니다.

LLM에게 질문과 함께 지식을 같이 제공하면, 아래와 같이 제공된 지식을 바탕으로 답변을 생성합니다.

```{admonition} 질문 : (지식을 제공하면서) 빽다방 카페인이 높은 음료와 가격은?
:class: tip

빽다방에서 카페인이 높은 음료는 다음과 같습니다:
1. 빽사이즈 원조커피(ICED) - 564mg 고카페인, 가격: 4000원
2. 빽사이즈 원조커피 제로슈거(ICED) - 686mg 고카페인, 가격: 4000원

이 두 음료가 카페인이 가장 높습니다.
```

### 방법 #1. 대화 시작 시에 한 번에 모든 지식을 제공하기

대화 시작 시에 대화에 필요한 모든 지식을 제공하면 연관된 정보들을 한꺼번에 제공하므로 더 포괄적이고 일관된 답변이 가능하며 추가 검색이 필요하지 않습니다.
하지만 불필요한 정보까지 포함될 수 있어 토큰 낭비 가능성이 있습니다.

사람이 한 번에 기억하고 처리할 수 있는 정보의 양이 제한되어 있는 것처럼, LLM도 한 순간에 참고하고 처리할 수 있는 텍스트의 양이 정해져 있습니다.
이러한 단기 기억을 [Context Window](https://platform.openai.com/docs/models#context-window)라고 합니다.
OpenAI의 GPT-4o는 최대 128k 토큰까지 처리할 수 있습니다.
Context Window 제약으로 인해 LLM에 한 번에 제공할 수 있는 지식의 크기가 제한되며, Context Window를 넘어선 채팅은 중단됩니다.

LLM에서는 글자를 토큰이라는 숫자로 변환하여 처리합니다. [OpenAI Tokenizer](https://platform.openai.com/tokenizer)에 의하면 "안녕 세상아" 문자열은 5개의 토큰으로 변환되며,
"hello world" 문자열은 2개의 토큰으로 변환됩니다.

### 방법 #2. 매 질문마다 질문 맥락에 맞는 지식을 찾아서 제공하기 (RAG)

지식의 양이 방대할 경우, 매 질문마다 질문에 맞는 지식을 찾아서 제공하는 방법이 효율적일 수 있습니다.
RAG가 이에 해당합니다.

지식을 저장하고, 질문과 유사한 지식을 찾아주는 데이터베이스 서버 혹은 유틸리티를 Vector Store 라고 부릅니다.
어떠한 지식을 통으로 Vector Store에 저장할 수는 없습니다.

지식은 PDF, 워드, 엑셀, 이미지, 웹문서 등의 다양한 형태로 존재할텐데요.
Vector Store에 저장하기 위해서는 이를 텍스트로 전처리하는 과정이 필요합니다.
파일에 따라 텍스트로 변환하는 과정이 녹록치 않을 수 있습니다.
(Case by case가 많고 비용도 많이 들고 기술력이 많이 필요합니다.)

Vector Store 종류로는 [pgvector](https://github.com/pgvector/pgvector), [sqlite-vec](https://github.com/asg017/sqlite-vec), faiss, chroma, pinecone, weaviate, milvus 등이 있구요.
본 튜토리얼에서는 파이썬 리스트를 기반으로 Vector Store 컨셉을 간략하게 구현해보겠습니다.
